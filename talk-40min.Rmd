---
title: "Interpretable Machine Learning"
author: "Christoph Molnar"
date: "June 7, 2018"
output: 
  ioslides_presentation:
    widescreen: false
    smaller: false
css: styles.css
---


```{r, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, 
  warning = FALSE,
  message = FALSE, 
  fig.align='center', 
  out.width = '100%')
```

##  {.emphasizedabit}
A client wants you to predict data scientist salaries with machine learning.  

##

```{r, echo=FALSE, out.height = '100%', out.width=NULL}
knitr::include_graphics("images/wow.jpg")
```

# Let's predict data scientist salaries



## What is Machine Learning

Machine learning is a method for teaching computers to make and improve predictions or behaviours based on data.

```{r, echo=FALSE, out.height = '100%', out.width=NULL}
knitr::include_graphics("images/magic.jpg")
```


## Step 1: Find some data
Kaggle conducted an industry-wide survey of data scientists. 
https://www.kaggle.com/kaggle/kaggle-survey-2017

Information asked:  

- Compensation
- Demographics
- Job title
- Experience
- ...


<font size="2">Contains information from Kaggle ML and Data Science Survey, 2017, which is made available here under the Open Database License (ODbL).</font>



```{r load-data,include=FALSE}
library('mlr')
library('ggplot2')
library('tidyr')
library('lime')
source('code/prepare-kaggle-data.R')
```



## Step 2: Throw ML on your data
```{r learn}
library('mlr')
set.seed(42)
task = makeRegrTask(data = survey.dat, target = 'CompensationAmount')
lrn = makeLearner('regr.randomForest')
mod = train(lrn, task)
```

```{r, echo=FALSE}
knitr::include_graphics("images/comp-dog.gif")
```

## Step 3: Profit. {.center}

```{r, echo=FALSE, out.height=NULL, out.width='100%'}
knitr::include_graphics("images/done-here.gif")
```

##  {.emphasizedabit}

Client: "There is a problem with the model!"

## {.emphasizedabit}

"What problem?""

```{r, echo=FALSE, out.height='50%'}
knitr::include_graphics("images/Hide-the-pain-harold-phone.jpg")
```


## {.emphasizedabit}

 Client: "Model predicts high salaries for old yet unskilled people."

```{r, echo=FALSE, out.height='30%'}
knitr::include_graphics("images/age.jpeg")
```



# Partial Dependence Plots

##

```{r}
library("iml")
X = survey.dat[-which(names(survey.dat) == "CompensationAmount")]
predictor = Predictor$new(mod, data = X, y = getTaskTargets(task))
pd = Partial$new(predictor, feature ='Age')
pd$plot() 
```


<font size="2">Goldstein, A., Kapelner, A., Bleich, J., & Pitkin, E. (2013). Peeking Inside the Black Box: Visualizing Statistical Learning with Plots of Individual Conditional Expectation, 1–22. https://doi.org/10.1080/10618600.2014.907095 </font>

<font size="2">Friedman, J. H. (1999). Greedy Function Approximation : A Gradient Boosting Machine. North, 1(3), 1–10. https://doi.org/10.2307/2699986 </font>


##  {.emphasizedabit}

Client: "We want to understand the model better!"

# Permutation feature importance

##

```{r, warning=FALSE}
feat.imp = FeatureImp$new(predictor, loss = 'mae')
feat.imp$plot()
```

<font size="2">Breiman, Leo. "Random forests." Machine learning 45.1 (2001): 5-32. </font>

## Gender?!
```{r, echo=FALSE, out.width='80%'}
knitr::include_graphics("images/big-mistake.png")
```

##

```{r}
pdp = Partial$new(predictor, feature="Gender")
pdp$plot() 
```


##

```{r, echo=FALSE}
knitr::include_graphics("images/hidden-pain-bias.jpg")
```

##

```{r, echo=FALSE, out.width='40%'}
knitr::include_graphics("images/angry.gif")
```



# LIME

## 
```{r, echo=FALSE}
set.seed(44)
```


```{r}
explanation = LocalModel$new(predictor)
explanation$explain(X[15, ])
explanation$plot()
```


<font size="2">Ribeiro, M. T., Singh, S., & Guestrin, C. (2016). “Why Should I Trust You?”: Explaining the Predictions of Any Classifier. Retrieved from http://arxiv.org/abs/1602.04938</font>

# Shapley

## 
```{r}
set.seed(44)
```


```{r}
explanation = Shapley$new(predictor)
explanation$explain(X[15, ])
explanation$plot()
```

# Some Theory


## The Problem

```{r, echo=FALSE, fig.align='center'}
knitr::include_graphics("images/black-box.png")
```

## The Problem

- Opaque decision making by machine learning algorithms
- Bias and mistakes in the data
- Trust as a user: Should I start a therapy with sever side effects, because the machine said so?
- Debugging as a practitioner: Why did the algorithm miss-classify sample X? Did it learn generalizable features? 


## What is interpretability?

Interpretability is the degree to which a human can understand the cause of a decision. 
But we don't really have a good way to measure that really. Not as easy as benchmarking ML algorithms. 

Interpretability is also a mean to look at the data and possible issues with them.

<font size="2">Miller, Tim. 2017. “Explanation in Artificial Intelligence: Insights from the Social Sciences.” arXiv Preprint arXiv:1706.07269.</font>

## When do we need interpretability?

We need interpretability when the loss function does not cover all constraints. 

- Decisions about humans.
- Critical applications that decide about life and death.
- Newly developed systems with unknown consequences.
- Models using proxies instead of causal inputs.
- Debugging the models.
- Increasing trust.

### Examples where we need interpretability

- Credit scores
- Autonomous vehicles
- Medical recommendations
- Automated diagnoses

## When don't we need interpretability?

When we can capture everything in the loss function and the data collection.
only causal relationships.
perfect operationalization of features.

Things that work well. 
Well defined problems

## Example: Digit recognition

![](images/mnist.jpeg)

## What tools do we have?

- Intrinsically interpretable models
- Tools for analyzing specific black box models
- Model-agnostic tools: analysing any black box model
- Interpretability through making data explainable


## Interpretable Models

```{r, echo = FALSE}
knitr::include_graphics("images/black-box.png")
```

## Interpretable Models

```{r, echo=FALSE}
knitr::include_graphics("images/white-box.png")
```



## Interpretable Models

- Decision trees
- Decision rules
- Generalized regression models
- ...

## Model-specific methods

```{r, echo=FALSE}
knitr::include_graphics("images/black-box.png")
```


## Model-specific methods

```{r, echo=FALSE}
knitr::include_graphics("images/specific-black-box.png")
```




## Model-specific methods

- RandomForestExplainer
- Visualizing activations of neural networks
- 


TODO: add images from papers

## Model-agnostic methods

```{r, echo=FALSE}
knitr::include_graphics("images/black-box.png")
```


## Model-agnostic methods

```{r, echo=FALSE}
knitr::include_graphics("images/agnostic-black-box.png")
```



## Model-agnostic methods

- Partial dependence plots
- LIME
- ...


## Example-focused methods

```{r, echo=FALSE}
knitr::include_graphics("images/black-box.png")
```




## Example-focused methods
```{r, echo=FALSE}
knitr::include_graphics("images/data-box.png")
```


## Example-focused methods

- Prototypes: What is the prototype of a certain class
- Criticisms: Which data points are unusual for a certain class. 
- Counterfactuals: What do I need to change for an instance to change the prediction?
- Anchors: What features do I have to anchor to keep prediction the same?

TODO: Cite


## Outcomes of interpretability method:

- Model
- Data point
- Interpretable unit (like a weight)
- Summary statistic for a feature

## Interested in learning more?

Read my book about "Interpretable Machine Learning"
https://christophm.github.io/interpretable-ml-book/

```{r, echo=FALSE, fig.align='center', out.width='30%'}
knitr::include_graphics("images/book.jpg")
```

